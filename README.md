# Pruning for Large Language Models


I am currently focusing on pruning large language models, including


- üìÑ **Papers**
  - [Survey](#survey)
  - [Unstructured Pruning](#unstructured-pruning)
  - [Structured Pruning](#structured-pruning)
  - [Semi-structured Pruning](#semi-structured-pruning)
  

- üõ†Ô∏è **Application**
  
 
  
<strong> Last Update: 2025/12/2 </strong>





<a name="Surveys" />

## Papers
### Surveys
- [2024.11] Large Language Models in Operations Research: Methods, Applications, and Challenges, TACL [[Paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482)]
- [2024.01] Model Compression and Efficient Inference for Large Language Models: A Survey, arXiv [[Paper](https://openreview.net/forum?id=rNoHBvNzHn)]

### Unstructured Pruning
- [2023.07] SparseGPT: Massive Language Models Can be Accurately Pruned in One-shot, ICML [[Paper](https://dl.acm.org/doi/abs/10.5555/3618408.3618822)][[Code](https://github.com/IST-DASLab/sparsegpt)]
### Structured Pruning
- [2024.10] DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models, NIPS [[Paper](https://dl.acm.org/doi/abs/10.5555/3737916.3740221)]
- [2024.07] MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2407.11681)]
### Semi-structured Pruning
